{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9rwQTIa+zVcyPUew0D7FN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arachne0/tensorflow/blob/master/atari_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym==0.26.2"
      ],
      "metadata": {
        "id": "_c2r8LMwSJp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install 'gym[atari]'"
      ],
      "metadata": {
        "id": "bhUeatHGSJhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install 'gym[accept-rom-license]'"
      ],
      "metadata": {
        "id": "8nNDKMZRSZvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ale-py"
      ],
      "metadata": {
        "id": "CFtaK-xKFECk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade git+https://github.com/openai/gym"
      ],
      "metadata": {
        "id": "KbwsBe1gS8d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autorom"
      ],
      "metadata": {
        "id": "RBXJslFtS8gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gym[atari]"
      ],
      "metadata": {
        "id": "RJnUf5_cS_G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ale_py import ALEInterface\n",
        "\n",
        "ale = ALEInterface()"
      ],
      "metadata": {
        "id": "VbnlH5tF65DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ale_py.roms import Breakout\n",
        "\n",
        "ale.loadROM(Breakout)"
      ],
      "metadata": {
        "id": "aHMBmEtG5lWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten"
      ],
      "metadata": {
        "id": "w2F1U-KM5lX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
        "class DQN(tf.keras.Model):\n",
        "    def __init__(self, action_size, state_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
        "                            input_shape=state_size)\n",
        "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
        "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
        "        self.flatten = Flatten()\n",
        "        self.fc = Dense(512, activation='relu')\n",
        "        self.fc_out = Dense(action_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        q = self.fc_out(x)\n",
        "        return q"
      ],
      "metadata": {
        "id": "YQFpXu9RT-Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 브레이크아웃 예제에서의 DQN 에이전트\n",
        "class DQNAgent:\n",
        "    def __init__(self, action_size, state_size=(84, 84, 4)):\n",
        "        self.render = False\n",
        "\n",
        "        # 상태와 행동의 크기 정의\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # DQN 하이퍼파라미터\n",
        "        self.discount_factor = 0.99\n",
        "        self.learning_rate = 1e-4\n",
        "        self.epsilon = 1.\n",
        "        self.epsilon_start, self.epsilon_end = 1.0, 0.02\n",
        "        self.exploration_steps = 1000000.\n",
        "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
        "        self.epsilon_decay_step /= self.exploration_steps\n",
        "        self.batch_size = 32\n",
        "        self.train_start = 50000\n",
        "        self.update_target_rate = 10000\n",
        "\n",
        "        # 리플레이 메모리, 최대 크기 100,000\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        # 게임 시작 후 랜덤하게 움직이지 않는 것에 대한 옵션\n",
        "        self.no_op_steps = 30\n",
        "\n",
        "        # 모델과 타깃 모델 생성\n",
        "        self.model = DQN(action_size, state_size)\n",
        "        self.target_model = DQN(action_size, state_size)\n",
        "        self.optimizer = Adam(self.learning_rate, clipnorm=10.)\n",
        "        # 타깃 모델 초기화\n",
        "        self.update_target_model()\n",
        "\n",
        "        self.avg_q_max, self.avg_loss = 0, 0\n",
        "\n",
        "        self.writer = tf.summary.create_file_writer('summary/breakout_dqn')\n",
        "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
        "\n",
        "    # 타깃 모델을 모델의 가중치로 업데이트\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    # 입실론 탐욕 정책으로 행동 선택\n",
        "    def get_action(self, history):\n",
        "        history = np.float32(history / 255.0)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            q_value = self.model(history)\n",
        "            return np.argmax(q_value[0])\n",
        "\n",
        "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
        "    def append_sample(self, history, action, reward, next_history, dead):\n",
        "        self.memory.append((history, action, reward, next_history, dead))\n",
        "\n",
        "    # 텐서보드에 학습 정보를 기록\n",
        "    def draw_tensorboard(self, score, step, episode):\n",
        "        with self.writer.as_default():\n",
        "            tf.summary.scalar('Total Reward/Episode', score, step=episode)\n",
        "            tf.summary.scalar('Average Max Q/Episode',\n",
        "                              self.avg_q_max / float(step), step=episode)\n",
        "            tf.summary.scalar('Duration/Episode', step, step=episode)\n",
        "            tf.summary.scalar('Average Loss/Episode',\n",
        "                              self.avg_loss / float(step), step=episode)\n",
        "\n",
        "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
        "    def train_model(self):\n",
        "        if self.epsilon > self.epsilon_end:\n",
        "            self.epsilon -= self.epsilon_decay_step\n",
        "\n",
        "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        history = np.array([sample[0][0] / 255. for sample in batch],\n",
        "                           dtype=np.float32)\n",
        "        actions = np.array([sample[1] for sample in batch])\n",
        "        rewards = np.array([sample[2] for sample in batch])\n",
        "        next_history = np.array([sample[3][0] / 255. for sample in batch],\n",
        "                                dtype=np.float32)\n",
        "        dones = np.array([sample[4] for sample in batch])\n",
        "\n",
        "        # 학습 파라메터\n",
        "        model_params = self.model.trainable_variables\n",
        "        with tf.GradientTape() as tape:\n",
        "            # 현재 상태에 대한 모델의 큐함수\n",
        "            predicts = self.model(history)\n",
        "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
        "            predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
        "\n",
        "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
        "            target_predicts = self.target_model(next_history)\n",
        "\n",
        "            # 벨만 최적 방정식을 구성하기 위한 타깃과 큐함수의 최대 값 계산\n",
        "            max_q = np.amax(target_predicts, axis=1)\n",
        "            targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
        "\n",
        "            # 후버로스 계산\n",
        "            error = tf.abs(targets - predicts)\n",
        "            quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
        "            linear_part = error - quadratic_part\n",
        "            loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
        "\n",
        "            self.avg_loss += loss.numpy()\n",
        "\n",
        "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
        "        grads = tape.gradient(loss, model_params)\n",
        "        self.optimizer.apply_gradients(zip(grads, model_params))"
      ],
      "metadata": {
        "id": "C5PQ2ovn5lZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습속도를 높이기 위해 흑백화면으로 전처리\n",
        "def pre_processing(observe):\n",
        "    processed_observe = np.uint8(\n",
        "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
        "    return processed_observe\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 환경과 DQN 에이전트 생성\n",
        "    env = gym.make('BreakoutDeterministic-v4')\n",
        "    agent = DQNAgent(action_size=3)\n",
        "\n",
        "    global_step = 0\n",
        "    score_avg = 0\n",
        "    score_max = 0\n",
        "\n",
        "    # 불필요한 행동을 없애주기 위한 딕셔너리 선언\n",
        "    action_dict = {0:1, 1:2, 2:3, 3:3}\n",
        "\n",
        "    num_episode = 50000\n",
        "    for e in range(num_episode):\n",
        "        done = False\n",
        "        dead = False\n",
        "\n",
        "        step, score, start_life = 0, 0, 5\n",
        "        # env 초기화\n",
        "        observe = env.reset()\n",
        "\n",
        "        # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
        "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
        "            #print(env.step(1))\n",
        "            observe, _, _, _,_ = env.step(1)\n",
        "\n",
        "        # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
        "        state = pre_processing(observe)\n",
        "        history = np.stack((state, state, state, state), axis=2)\n",
        "        history = np.reshape([history], (1, 84, 84, 4))\n",
        "\n",
        "        while not done:\n",
        "            if agent.render:\n",
        "                env.render()\n",
        "            global_step += 1\n",
        "            step += 1\n",
        "\n",
        "            # 바로 전 history를 입력으로 받아 행동을 선택\n",
        "            action = agent.get_action(history)\n",
        "            # 1: 정지, 2: 왼쪽, 3: 오른쪽\n",
        "            real_action = action_dict[action]\n",
        "\n",
        "            # 죽었을 때 시작하기 위해 발사 행동을 함\n",
        "            if dead:\n",
        "                action, real_action, dead = 0, 1, False\n",
        "\n",
        "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
        "            observe, reward, done, truncated ,info = env.step(real_action)\n",
        "            print(e)\n",
        "            #print(info)\n",
        "            # 각 타임스텝마다 상태 전처리\n",
        "            next_state = pre_processing(observe)\n",
        "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
        "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
        "\n",
        "            agent.avg_q_max += np.amax(agent.model(np.float32(history / 255.))[0])\n",
        "\n",
        "            if start_life > info['lives']:\n",
        "                dead = True\n",
        "                start_life = info['lives']\n",
        "\n",
        "            score += reward\n",
        "            reward = np.clip(reward, -1., 1.)\n",
        "            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
        "            agent.append_sample(history, action, reward, next_history, dead)\n",
        "\n",
        "            # 리플레이 메모리 크기가 정해놓은 수치에 도달한 시점부터 모델 학습 시작\n",
        "            if len(agent.memory) >= agent.train_start:\n",
        "                agent.train_model()\n",
        "                # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
        "                if global_step % agent.update_target_rate == 0:\n",
        "                    agent.update_target_model()\n",
        "\n",
        "            if dead:\n",
        "                history = np.stack((next_state, next_state,\n",
        "                                    next_state, next_state), axis=2)\n",
        "                history = np.reshape([history], (1, 84, 84, 4))\n",
        "            else:\n",
        "                history = next_history\n",
        "\n",
        "            if done:\n",
        "                # 각 에피소드 당 학습 정보를 기록\n",
        "                if global_step > agent.train_start:\n",
        "                    agent.draw_tensorboard(score, step, e)\n",
        "\n",
        "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
        "                score_max = score if score > score_max else score_max\n",
        "\n",
        "                log = \"episode: {:5d} | \".format(e)\n",
        "                log += \"score: {:4.1f} | \".format(score)\n",
        "                log += \"score max : {:4.1f} | \".format(score_max)\n",
        "                log += \"score avg: {:4.1f} | \".format(score_avg)\n",
        "                log += \"memory length: {:5d} | \".format(len(agent.memory))\n",
        "                log += \"epsilon: {:.3f} | \".format(agent.epsilon)\n",
        "                log += \"q avg : {:3.2f} | \".format(agent.avg_q_max / float(step))\n",
        "                log += \"avg loss : {:3.2f}\".format(agent.avg_loss / float(step))\n",
        "                print(log)\n",
        "\n",
        "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
        "\n",
        "        # 1000 에피소드마다 모델 저장\n",
        "        if e % 1000 == 0:\n",
        "            agent.model.save_weights(\"./save_model/model\", save_format=\"tf\")"
      ],
      "metadata": {
        "id": "ShvLYEdw52B-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}